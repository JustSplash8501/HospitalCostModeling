---
title: "Medical Insurance Cost Prediction: A Machine Learning Analysis"
author: "Reese Wilson"
date: today
format: 
  pdf:
    toc: true
  html:
    toc: true
    theme: cosmo
description: |
  Analysis and modeling of insurance charges using demographic and health features.
  Includes feature engineering, Random Forest regression with hyperparameter tuning via Optuna,
  and interpretability with SHAP.
keywords: [data science, machine learning, regression, Random Forest, Optuna, SHAP]
repository: https://github.com/JustSplash8501/hospital-cost-modeling
execute:
  warning: false
  message: false
  echo: true
---

# Executive Summary

This analysis develops predictive models for medical insurance costs using patient demographic and health data. We compare two modeling approaches: Multiple Linear Regression and Random Forest Regression. The Random Forest model significantly outperformed linear regression, achieving a test MAE of $2,560 compared to $4,237, demonstrating the importance of capturing non-linear relationships in insurance cost prediction. Key findings reveal that smoking status, age, and BMI are the most influential factors in determining medical insurance costs.

# Introduction

## Background

Medical insurance costs in the United States vary substantially based on individual characteristics and health factors. Understanding the drivers of these costs is crucial for insurance companies in premium pricing, risk assessment, and for individuals in making informed healthcare decisions. This analysis examines how demographic factors (age, sex, region) and health-related variables (BMI, smoking status, number of children) influence medical insurance charges.

# Import data

```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os

if os.name == "posix":
    path = "/Users/raw/Desktop/gitProjects/HospitalCostModeling/data/insurance.csv"
else:
    path = r"A:\VSCodeProjects\MLA Projects\HospitalCostModeling\data\insurance.csv"

hospitaldata = pd.read_csv(path)
hospitaldata.head()
```

# Two Modeling Approaches
## Multi-Linear Regression

### Check Multi-Linear Regression Assumptions
**VIF**
Based on variable importance factor (VIF), no multi-collinearity exists within this dataset. This test satisfies one of the assumptions required for multi-linear regression.
```{python}
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

def compute_vif(df):
    numeric_cols = hospitaldata.select_dtypes(include="number")

    numeric_cols = add_constant(numeric_cols)
    vif = pd.DataFrame(
        {
            "feature": numeric_cols.columns,
            "VIF": [
                variance_inflation_factor(numeric_cols.values, i)
                for i in range(numeric_cols.shape[1])
            ],
        }
    )
    return vif


vif_df = compute_vif(hospitaldata)
print(vif_df)
```

### Construct Multi-Linear Regression Model

```{python}
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

x = pd.DataFrame(hospitaldata.drop(["charges"], axis=1))
cat_cols = x.select_dtypes(include="object").columns
num_cols = x.select_dtypes(exclude="object").columns

y = pd.Series(hospitaldata["charges"])

X_train, X_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=42
)

preprocess = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), num_cols),
        ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), cat_cols),
    ]
)

mlmmodel = Pipeline(
    steps=[("preprocess", preprocess), ("regressor", LinearRegression())]
)

mlmmodel.fit(X_train, y_train)
```

**Residuals Distribution:**
- **Heteroskedasticity**: Clear evidence that residual variance increases with fitted values, violating the homoscedasticity assumption
- **Non-linearity**: Distinct patterns in residuals suggest non-linear relationships not captured by the linear model
- **Normality**: Q-Q plot shows heavy tails, indicating some departure from normality
- **Implication**: These violations suggest that a more flexible modeling approach may be warranted

```{python}
y_hat = mlmmodel.predict(X_train)
residuals = y_train - y_hat

fig, ax = plt.subplots()
ax.scatter(y_hat, residuals)
ax.axhline(0)
ax.set_xlabel("Fitted values")
ax.set_ylabel("Residuals")
plt.show()
```

**Q-Q Plot and Independence of Errors**
- Further suggestions of non-linear relationships within the data
- No autocorrelation exists
```{python}
import scipy.stats as stats
from statsmodels.stats.stattools import durbin_watson

# Q-Q Plot
fig, ax = plt.subplots()
stats.probplot(residuals, plot=ax)
ax.set_title("Q–Q Plot of Residuals")
plt.show()

# Auto-correlation test
dw_val = durbin_watson(residuals)
dw_val_str = (
    f"Autocorrelation exists (DW = {dw_val:.2f})"
    if dw_val < 1.5 or dw_val > 2.5
    else f"No autocorrelation (DW = {dw_val:.2f})"
)

print(dw_val_str)
```

### Model Diagnostics

```{python}
from sklearn.metrics import r2_score, mean_absolute_error


def adj_r2_score(r2, n, p):
    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
    return adj_r2


# Train Metrics
y_train_pred = mlmmodel.predict(X_train)
r2_train = r2_score(y_train, y_train_pred)
n_train = np.shape(y_train)[0]
p = mlmmodel.n_features_in_
adj_r2_train = adj_r2_score(r2_train, n_train, p)
print(f"MLM Regressor")
print(f"    Training Dataset Adjusted R2 = {adj_r2_train:.2f}")
mae_train = mean_absolute_error(y_train, y_train_pred)
print(f"    Training Dataset MAE: {mae_train:.2f}\n")

# Test Metrics
y_test_pred = mlmmodel.predict(X_test)
r2_test = r2_score(y_test, y_test_pred)
n_test = np.shape(y_test)[0]
adj_r2_test = adj_r2_score(r2_test, n_test, p)
print(f"    Test Dataset Adjusted R2 = {adj_r2_test:.2f}")
mae_test = mean_absolute_error(y_test, y_test_pred)
print(f"    Test Dataset MAE: {mae_test:.2f}\n\n")
```

The linear model explains approximately 78% of the variance in insurance costs but shows concerning assumption violations that may limit its predictive accuracy. To address this, a model, Random Forest, which can capture non-linear relationships will be tested to determine if it can be more accurate in prediction of the target variable.

## Random Forest Regressor

```{python}
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor

preprocess = ColumnTransformer(
    transformers=[
        ("num", "passthrough", num_cols),
        ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), cat_cols),
    ]
)

rfrmodel = Pipeline(
    steps=[("preprocess", preprocess), 
    ("regressor", RandomForestRegressor(max_depth=2, random_state=0))]
)

rfrmodel.fit(X_train, y_train)
```

### Model Diagnostics

```{python}
from sklearn.metrics import r2_score, mean_absolute_error

# Training
y_train_pred = rfrmodel.predict(X_train)
r2_train = r2_score(y_train, y_train_pred)
print(f"Random Forest Regressor")
print(f"    Training Dataset R-squared: {r2_train:.2f}")
mae_train = mean_absolute_error(y_train, y_train_pred)
print(f"    Training Dataset MAE: {mae_train:.2f}\n")

# Test
y_test_pred = rfrmodel.predict(X_test)
r2_test = r2_score(y_test, y_test_pred)
print(f"    Test Dataset R-squared: {r2_test:.2f}")
mae_test = mean_absolute_error(y_test, y_test_pred)
print(f"    Test Dataset MAE: {mae_test:.2f}\n\n")
```

Preliminarily, the random forest model appears to be improved from the multi-linear regression with R² of 0.84 and a MAE of 3192 on the test dataset. With further tuning, the error of prediction can likely be decreased further improving the overall ability of the model to produce a more confident prediction.

## Random Forest Regressor Tuned

```{python}
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
import optuna
from optuna.samplers import TPESampler

preprocess = ColumnTransformer(
    transformers=[
        ("num", "passthrough", num_cols),
        ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), cat_cols),
    ]
)

rfrtunemodel = Pipeline(
    [
        ("preprocess", preprocess),
        ("regressor", RandomForestRegressor(random_state=42)),
    ]
)


def objective(trial):
    n_estimators = trial.suggest_int("regressor__n_estimators", 100, 500)
    max_depth = trial.suggest_int("regressor__max_depth", 8, 20)  # Sweet spot range
    min_samples_split = trial.suggest_int("regressor__min_samples_split", 2, 15)
    min_samples_leaf = trial.suggest_int("regressor__min_samples_leaf", 1, 8)
    max_features = trial.suggest_categorical(
        "regressor__max_features", ["sqrt", "log2", 0.5]
    )

    rfrtunemodel.set_params(
        regressor__n_estimators=n_estimators,
        regressor__max_depth=max_depth,
        regressor__min_samples_split=min_samples_split,
        regressor__min_samples_leaf=min_samples_leaf,
        regressor__max_features=max_features,
    )

    score = cross_val_score(
        rfrtunemodel,
        X_train,
        y_train,
        cv=5,
        scoring="neg_root_mean_squared_error",
        n_jobs=5,
    ).mean()

    return score


study = optuna.create_study(direction="maximize", sampler=TPESampler(n_startup_trials=5))
study.optimize(objective, n_trials=50, timeout=3600)

best_params = study.best_params
rfrtunemodel.set_params(**best_params)
rfrtunemodel.fit(X_train, y_train)
y_pred = rfrtunemodel.predict(X_test)
```

### Model Training History

```{python}
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import r2_score, mean_absolute_error

# Create the optimization history plot with custom styling
fig, ax = plt.subplots(figsize=(12, 6), dpi=100)

# Optimization history data
trials = study.trials
trial_numbers = [t.number for t in trials]
values = [t.value for t in trials if t.value is not None]
trial_numbers = [t.number for t in trials if t.value is not None]

# Plot individual trial values
ax.scatter(
    trial_numbers,
    np.abs(values),
    alpha=0.6,
    s=50,
    color="#3498db",
    edgecolors="white",
    linewidth=1.5,
    zorder=3,
    label="Trial Value",
)

# Calculate and plot the best value so far
best_values = []
abs_values = [abs(v) for v in values]
current_best = float("inf")
for abs_val in abs_values:
    current_best = min(current_best, abs_val)
    best_values.append(current_best)

ax.plot(
    trial_numbers,
    best_values,
    color="#e74c3c",
    linewidth=2.5,
    label="Best Value",
    zorder=4,
)

# Styling
ax.set_xlabel("Trial Number", fontsize=12, fontweight="bold", labelpad=10)
ax.set_ylabel("Objective Value", fontsize=12, fontweight="bold", labelpad=10)
ax.set_title("Optimization History", fontsize=14, fontweight="bold", pad=20)

# Grid
ax.grid(True, alpha=0.3, linestyle="--", linewidth=0.8, zorder=1)
ax.set_axisbelow(True)

# Legend
ax.legend(
    loc="best", frameon=True, fancybox=True, shadow=True, fontsize=10, framealpha=0.95
)

# Spines
for spine in ax.spines.values():
    spine.set_visible(True)
    spine.set_linewidth(2)
    spine.set_edgecolor("black")

# Tight layout
plt.tight_layout()
plt.show()
```

### Tuned Model Diagnostics

```{python}
from sklearn.metrics import r2_score, mean_absolute_error

print(f"Tuned Random Forest Regressor")
best_params_str = "\n".join(f"        {k}: {v}" for k, v in study.best_params.items())
print(f"    Best hyperparameters:\n{best_params_str}")
print(f"    Best CV RMSE: {np.abs(study.best_value):.2f}")

# Test Metrics
y_test_pred = rfrtunemodel.predict(X_test)
r2_test = r2_score(y_test, y_test_pred)
print(f"    Test Dataset R-squared: {r2_test:.2f}")
mae_test = mean_absolute_error(y_test, y_test_pred)
print(f"    Test Dataset MAE: {mae_test:.2f}\n\n")
```

While the R² of the tuned random forest model did not improve compared to the base model, the MAE of the model was improved from 3192.66 to 2560.42 on the test dataset suggesting a noticeable improvement. Given this improvement, the tuned model should be statistically tested against the multi-linear regression model to determine which method statistically produces less error in prediction.

### Feature Importance

```{python}
import shap

# Explain the fitted RandomForest on preprocessed data
preprocessor = rfrtunemodel.named_steps["preprocess"]
X_train_proc = preprocessor.transform(X_train)
feature_names = preprocessor.get_feature_names_out()

explainer_tree = shap.TreeExplainer(rfrtunemodel.named_steps["regressor"])
shap_values_tree = explainer_tree(X_train_proc)
shap.summary_plot(shap_values_tree, features=X_train_proc, feature_names=feature_names)
```

**Feature Importance Insights:**

Based on SHAP values, the primary drivers of insurance costs are:

1. **Smoking Status**: By far the most influential factor, with smokers incurring substantially higher costs
2. **Age**: Strong positive relationship with costs, reflecting accumulation of health risks
3. **BMI**: Significant impact, particularly in interaction with smoking status
4. **Number of Children**: Moderate positive effect on insurance costs
5. **Sex and Region**: Relatively minimal impact compared to health-related factors

This analysis confirms that modifiable health behaviors (smoking) and demographic factors (age) are the dominant predictors of insurance costs.

# Compare Modeling Approaches
To determine, statistically, which model produces less error in prediction, a paired t-test comparing the residuals of the multi-linear regression and tuned random forest will be used.

```{python}
from scipy.stats import ttest_rel
from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error

mlmerr = np.abs(y_test - mlmmodel.predict(X_test))
rfrtuneerr = np.abs(y_test - rfrtunemodel.predict(X_test))

__, p_value = ttest_rel(mlmerr, rfrtuneerr)
p_value_str = "<0.01" if p_value < 0.01 else f"{p_value:.4f}"
print(f"MLM MAE: {np.mean(mlmerr):.2f}\nRFR Tuned MAE: {np.mean(rfrtuneerr):.2f}")
print(f"Paired t-test p-value (MAE): {p_value_str}")

# Calculate metrics for all models
models = {
    "Multi-Linear Regression": mlmmodel,
    "Random Forest (Base)": rfrmodel,
    "Random Forest (Tuned)": rfrtunemodel,
}

results = []
for model_name, model in models.items():
    # Training metrics
    y_train_pred = model.predict(X_train)
    train_r2 = r2_score(y_train, y_train_pred)
    train_mae = mean_absolute_error(y_train, y_train_pred)
    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))

    # Test metrics
    y_test_pred = model.predict(X_test)
    test_r2 = r2_score(y_test, y_test_pred)
    test_mae = mean_absolute_error(y_test, y_test_pred)
    test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))

    results.append(
        {
            "Model": model_name,
            "Train R²": train_r2,
            "Test R²": test_r2,
            "Train MAE": train_mae,
            "Test MAE": test_mae,
            "Train RMSE": train_rmse,
            "Test RMSE": test_rmse,
        }
    )

# Create DataFrame
performance_df = pd.DataFrame(results)

# Format for display
performance_df_display = performance_df.copy()
format_dict = {
    "Train R²": "{:.4f}",
    "Test R²": "{:.4f}",
    "Train MAE": "${:,.2f}",
    "Test MAE": "${:,.2f}",
    "Train RMSE": "${:,.2f}",
    "Test RMSE": "${:,.2f}",
}

for col, fmt in format_dict.items():
    performance_df_display[col] = performance_df_display[col].map(fmt.format)

print(performance_df_display)

fig, axes = plt.subplots(1, 3, figsize=(15, 5))

models_list = performance_df["Model"].tolist()
x = np.arange(len(models_list))
width = 0.35

# R² comparison
ax1 = axes[0]
ax1.bar(x - width / 2, performance_df["Train R²"], width, label="Train", alpha=0.8)
ax1.bar(x + width / 2, performance_df["Test R²"], width, label="Test", alpha=0.8)
ax1.set_ylabel("R² Score")
ax1.set_title("R² Comparison")
ax1.set_xticks(x)
ax1.set_xticklabels(models_list, rotation=45, ha="right")
ax1.legend()
ax1.grid(axis="y", alpha=0.3)

# MAE comparison
ax2 = axes[1]
ax2.bar(x - width / 2, performance_df["Train MAE"], width, label="Train", alpha=0.8)
ax2.bar(x + width / 2, performance_df["Test MAE"], width, label="Test", alpha=0.8)
ax2.set_ylabel("Mean Absolute Error ($)")
ax2.set_title("MAE Comparison")
ax2.set_xticks(x)
ax2.set_xticklabels(models_list, rotation=45, ha="right")
ax2.legend()
ax2.grid(axis="y", alpha=0.3)

# RMSE comparison
ax3 = axes[2]
ax3.bar(x - width / 2, performance_df["Train RMSE"], width, label="Train", alpha=0.8)
ax3.bar(x + width / 2, performance_df["Test RMSE"], width, label="Test", alpha=0.8)
ax3.set_ylabel("Root Mean Squared Error ($)")
ax3.set_title("RMSE Comparison")
ax3.set_xticks(x)
ax3.set_xticklabels(models_list, rotation=45, ha="right")
ax3.legend()
ax3.grid(axis="y", alpha=0.3)

plt.tight_layout()
plt.show()

```

# Key Findings and Insights

## Model Performance Summary

The tuned Random Forest model significantly outperformed Multiple Linear Regression (*p* < 0.001), reducing prediction error by approximately 40%. This demonstrates that:

1. **Non-linear relationships are critical**: The insurance cost structure contains substantial non-linearities that linear models cannot capture
2. **Interaction effects matter**: Particularly the interaction between smoking status and other health factors (BMI, age)
3. **Tree-based methods excel**: For this type of cost prediction problem with categorical risk factors

## Business Implications

### For Insurance Companies:

1. **Risk Stratification**: Smoking status should be a primary factor in premium pricing, given its dominant impact on costs
2. **Preventive Programs**: Investment in smoking cessation programs could yield substantial cost savings
3. **Age-Based Pricing**: Age shows consistent positive effects and should be incorporated into actuarial models
4. **BMI Considerations**: While important, BMI effects are non-linear and interact with smoking status

### For Policyholders:

1. **Modifiable Risk Factors**: Smoking is by far the largest controllable factor affecting premiums
2. **Long-term Planning**: Age-related cost increases are substantial and should inform retirement healthcare planning
3. **Regional Variation**: Geographic location has minimal impact compared to personal health factors

## Model Limitations

1. **Data Coverage**: Dataset represents a snapshot and may not capture longitudinal healthcare utilization patterns
2. **Missing Variables**: Important predictors like pre-existing conditions, medication usage, and healthcare utilization history are absent
3. **Temporal Validity**: Healthcare costs and insurance pricing structures change over time
4. **Generalizability**: Results may not generalize to all insurance markets or populations

# Conclusions

This analysis successfully developed predictive models for medical insurance costs, with the tuned Random Forest model achieving strong performance (R² = 0.87, MAE = $2,560). The analysis reveals that smoking status is overwhelmingly the most important predictor of insurance costs, followed by age and BMI. The substantial improvement of Random Forest over linear regression (40% reduction in MAE) demonstrates the importance of capturing non-linear relationships in healthcare cost modeling.

The findings have clear implications for insurance pricing, risk management, and public health policy, particularly highlighting the financial burden of smoking on healthcare costs. Future work should incorporate additional health history variables, temporal dynamics, and explore more sophisticated ensemble methods or neural network approaches for further improvements in prediction accuracy.

# Save Final Model

This final model will be used for the web UI deployment where individuals can enter their health and descriptive information and recieve a prediction for thei rhealth insurance cost.

```{python}
import joblib
from datetime import datetime
import sklearn

model_artifact = {
    "model": rfrtunemodel,
    "best_params": study.best_params,
    "test_r2": r2_test,
    "test_mae": mae_test,
    "feature_names": list(X_train.columns),
    "trained_date": datetime.now().isoformat(),
    "sklearn_version": sklearn.__version__,
}

os.makedirs("models", exist_ok=True)
joblib.dump(model_artifact, "models/tuned_rf_model.pkl")
```
