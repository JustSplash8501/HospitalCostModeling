---
title: "Hospital Cost Data Modeling"
author: Reese Wilson
format: html
---


```{python}
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

import os
```

# Import data

```{python}
if os.name == "posix":
    path = "/Users/raw/Desktop/gitProjects/HospitalCostModeling/data/insurance.csv"
else:
    path = r"A:\VSCodeProjects\MLA Projects\HospitalCostModeling\data\insurance.csv"

hospitaldata = pd.read_csv(path)
hospitaldata.head()
```

# Two Modeling Approaches
## Multi-Linear Regression

### Check Multi-Linear Regression Assumptions
**VIF**
Based on variable importance factor (VIF), no multi-collinearity exists within this dataset. This test satisifies one of the assumption required for multi-linear regression.
```{python}
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

def compute_vif(df):
    numeric_cols = hospitaldata.select_dtypes(include="number")

    numeric_cols = add_constant(numeric_cols)
    vif = pd.DataFrame(
        {
            "feature": numeric_cols.columns,
            "VIF": [
                variance_inflation_factor(numeric_cols.values, i)
                for i in range(numeric_cols.shape[1])
            ],
        }
    )
    return vif


vif_df = compute_vif(hospitaldata)
print(vif_df)
```

### Consutruct Multi-Linear Regression Model

```{python}
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression

x = pd.DataFrame(hospitaldata.drop(["charges"], axis=1))
cat_cols = x.select_dtypes(include="object").columns
num_cols = x.select_dtypes(exclude="object").columns

y = pd.Series(hospitaldata["charges"])

X_train, X_test, y_train, y_test = train_test_split(
    x, y, test_size=0.2, random_state=42
)

preprocess = ColumnTransformer(
    transformers=[
        ("num", StandardScaler(), num_cols),
        ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), cat_cols),
    ]
)

mlmmodel = Pipeline(
    steps=[("preprocess", preprocess), ("regressor", LinearRegression())]
)

mlmmodel.fit(X_train, y_train)
```

**Residuals Distribution:**
- Clear heteroskedasticity
- indications of non-linear relationships
- Some sort of latent structure exists
- High-leverage regions

```{python}
y_hat = mlmmodel.predict(X_train)
residuals = y_train - y_hat

fig, ax = plt.subplots()
ax.scatter(y_hat, residuals)
ax.axhline(0)
ax.set_xlabel("Fitted values")
ax.set_ylabel("Residuals")
plt.show()
```

**Q-Q Plot and Independence of Errors**
- Further suggestions of non-linear relationships within the data
- No autocorrelation exists
```{python}
import scipy.stats as stats
from statsmodels.stats.stattools import durbin_watson

# Q-Q Plot
fig, ax = plt.subplots()
stats.probplot(residuals, plot=ax)
ax.set_title("Qâ€“Q Plot of Residuals")
plt.show()

# Auto-correlation test
dw_val = durbin_watson(residuals)
dw_val_str = (
    f"Autocorrelation exists (DW = {dw_val:.2f})"
    if dw_val < 1.5 or dw_val > 2.5
    else f"No autocorrelation (DW = {dw_val:.2f})"
)

print(dw_val_str)
```

### Model Diagnostics

```{python}
from sklearn.metrics import r2_score, mean_absolute_error


def adj_r2_score(r2, n, p):
    adj_r2 = 1 - (1 - r2) * (n - 1) / (n - p - 1)
    return adj_r2


# Train Metrics
y_train_pred = mlmmodel.predict(X_train)
r2_train = r2_score(y_train, y_train_pred)
n_train = np.shape(y_train)[0]
p = mlmmodel.n_features_in_
adj_r2_train = adj_r2_score(r2_train, n_train, p)
print(f"MLM Regressor")
print(f"    Training Dataset Adjusted R2 = {adj_r2_train:.2f}")
mae_train = mean_absolute_error(y_train, y_train_pred)
print(f"    Training Dataset MAE: {mae_train:.2f}\n")

# Test Metrics
y_test_pred = mlmmodel.predict(X_test)
r2_test = r2_score(y_test, y_test_pred)
n_test = np.shape(y_test)[0]
adj_r2_test = adj_r2_score(r2_test, n_test, p)
print(f"    Test Dataset Adjusted R2 = {adj_r2_test:.2f}")
mae_train = mean_absolute_error(y_test, y_test_pred)
print(f"    Test Dataset MAE: {mae_train:.2f}\n\n")
```

Ultimately, despite the moderate variance explained in this dataset by the multilinear regression (Adj. R^2 = 0.78) on the test dataset, there are indications of assumptions being violated based on residual analysis. To address this, a model, randomForest, which can capture non-linear relationships will be tested to determine if it can be more accurate in prediction of the target variable.

## randomForst Regressor

```{python}
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor

preprocess = ColumnTransformer(
    transformers=[
        ("num", "passthrough", num_cols),
        ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), cat_cols),
    ]
)

rfrmodel = Pipeline(
    steps=[("preprocess", preprocess), 
    ("regressor", RandomForestRegressor(max_depth=2, random_state=0))]
)

rfrmodel.fit(X_train, y_train)
```

### Model Diagnostics

```{python}
from sklearn.metrics import r2_score, mean_absolute_error

# Training
y_train_pred = rfrmodel.predict(X_train)
r2_train = r2_score(y_train, y_train_pred)
print(f"randomForest Regressor")
print(f"    Training Dataset R-squared: {r2_train:.2f}")
mae_train = mean_absolute_error(y_train, y_train_pred)
print(f"    Training Dataset MAE: {mae_train:.2f}\n")

# Test
y_test_pred = rfrmodel.predict(X_test)
r2_test = r2_score(y_test, y_test_pred)
print(f"    Test Dataset R-squared: {r2_test:.2f}")
mae_train = mean_absolute_error(y_test, y_test_pred)
print(f"    Test Dataset MAE: {mae_train:.2f}\n\n")
```
Preliminarily, the random forest model appears to be improved from the multi-linear regression with R^2 of 0.84 and a MAE of 3192 on the test dataset. With further tuning, the error of prediction can likely be decreased further improving the overall ability of the model to produce a more confident prediction.

## randomForest Regressor Tuned

```{python}
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score
from sklearn.ensemble import RandomForestRegressor
import optuna
from optuna.samplers import TPESampler

preprocess = ColumnTransformer(
    transformers=[
        ("num", "passthrough", num_cols),
        ("cat", OneHotEncoder(drop="first", handle_unknown="ignore"), cat_cols),
    ]
)

rfrtunemodel = Pipeline(
    [
        ("preprocess", preprocess),
        ("regressor", RandomForestRegressor(random_state=42)),
    ]
)


def objective(trial):
    n_estimators = trial.suggest_int("regressor__n_estimators", 100, 1000)
    max_depth = trial.suggest_int("regressor__max_depth", 5, 30)
    min_samples_split = trial.suggest_int("regressor__min_samples_split", 2, 10)
    min_samples_leaf = trial.suggest_int("regressor__min_samples_leaf", 1, 5)
    max_features = trial.suggest_categorical(
        "regressor__max_features", ["sqrt", "log2"]
    )

    rfrtunemodel.set_params(
        regressor__n_estimators=n_estimators,
        regressor__max_depth=max_depth,
        regressor__min_samples_split=min_samples_split,
        regressor__min_samples_leaf=min_samples_leaf,
        regressor__max_features=max_features,
    )

    score = cross_val_score(
        rfrtunemodel, X_train, y_train, cv=5, scoring="neg_mean_absolute_error", n_jobs = 5
    ).mean()

    return score

study = optuna.create_study(direction="maximize", sampler=GPSampler(n_startup_trials=5))
study.optimize(objective, n_trials=50, timeout=3600)

best_params = study.best_params
rfrtunemodel.set_params(**best_params)
rfrtunemodel.fit(X_train, y_train)
y_pred = rfrtunemodel.predict(X_test)
```

### Model Training History

```{python}
import matplotlib.pyplot as plt
import numpy as np
from sklearn.metrics import r2_score, mean_absolute_error
import optuna.visualization.matplotlib as ovm

# Create the optimization history plot with custom styling
fig, ax = plt.subplots(figsize=(12, 6), dpi=100)

# Optimization history data
trials = study.trials
trial_numbers = [t.number for t in trials]
values = [t.value for t in trials if t.value is not None]
trial_numbers = [t.number for t in trials if t.value is not None]

# Plot individual trial values
ax.scatter(
    trial_numbers,
    np.abs(values),
    alpha=0.6,
    s=50,
    color="#3498db",
    edgecolors="white",
    linewidth=1.5,
    zorder=3,
    label="Trial Value",
)

# Calculate and plot the best value so far
best_values = []
abs_values = [abs(v) for v in values]
current_best = float("inf")
for abs_val in abs_values:
    current_best = min(current_best, abs_val)
    best_values.append(current_best)

ax.plot(
    trial_numbers,
    best_values,
    color="#e74c3c",
    linewidth=2.5,
    label="Best Value",
    zorder=4,
)

# Styling
ax.set_xlabel("Trial Number", fontsize=12, fontweight="bold", labelpad=10)
ax.set_ylabel("Objective Value", fontsize=12, fontweight="bold", labelpad=10)
ax.set_title("Optimization History", fontsize=14, fontweight="bold", pad=20)

# Grid
ax.grid(True, alpha=0.3, linestyle="--", linewidth=0.8, zorder=1)
ax.set_axisbelow(True)

# Legend
ax.legend(
    loc="best", frameon=True, fancybox=True, shadow=True, fontsize=10, framealpha=0.95
)

# Spines
for spine in ax.spines.values():
    spine.set_visible(True)
    spine.set_linewidth(2)
    spine.set_edgecolor("black")

# Tight layout
plt.tight_layout()
plt.show()
```

### Tuned Model Diagnostics

```{python}
from sklearn.metrics import r2_score, mean_absolute_error

print(f"Tuned randomForest Regressor")
best_params_str = "\n".join(f"        {k}: {v}" for k, v in study.best_params.items())
print(f"    Best hyperparameters:\n{best_params_str}")
print(f"    Best CV MAE: {np.abs(study.best_value):.2f}")

# Test Metrics
y_test_pred = rfrtunemodel.predict(X_test)
r2_test = r2_score(y_test, y_test_pred)
print(f"    Test Dataset R-squared: {r2_test:.2f}")
mae_train = mean_absolute_error(y_test, y_test_pred)
print(f"    Test Dataset MAE: {mae_train:.2f}\n\n")
```
While the R^2 of the tuned random forest model did not improve compared to the base model, the MAE of the model was improved from 3192.66 to 2560.42 on the test dataset suggesting a noticeable improved. Given this improvement, the tuned model should be statistically tested against the multi-linear regression model to determine which method statistically produces less error in prediction.

# Compare Modeling Approaches
To determine, statistcally, which model produces less error in prediction, a paired T-test comparing the residuals of the multi-linear regression and tuned random forest will be used.

```{python}
import numpy as np
from joblib import Parallel, delayed
from sklearn.utils import resample
from sklearn.metrics import mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split

# Use a paired T-test to compare MAE errors
from scipy.stats import ttest_rel
from sklearn.metrics import mean_absolute_error, r2_score

mlmerr = np.abs(y_test - mlmmodel.predict(X_test))
rfrtuneerr = np.abs(y_test - rfrtunemodel.predict(X_test))

__, p_value = ttest_rel(mlmerr, rfrtuneerr)
p_value_str = "<0.01" if p_value < 0.01 else f"{p_value:.4f}"
print(f"MLM MAE: {np.mean(mlmerr):.2f}\nRFR Tuned MAE: {np.mean(rfrtuneerr):.2f}")
print(f"Paired t-test p-value (MAE): {p_value_str}")
```
The tuned random forest model produced less error (*P* < 0.01) than the multi-linear regression model indicating the random forest model better captured the relationship between individuals basic health information and their hostpital charges.